{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "import pickle\n",
    "import timeit\n",
    "from random import *\n",
    "import random\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start With Existing Data Here [Acronym, Meaning, Content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_pickle('DataSets/custom_acr_mean_cont.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove people in the following dataset:\n",
    "This dataset was created by training an LSTM classifier to classify people in the wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ppl_remove = pd.read_pickle('DataSets/people_to_remove.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510519\n",
      "410954\n"
     ]
    }
   ],
   "source": [
    "print(len(full_df))\n",
    "full_df = full_df[~full_df['Meaning'].isin(list(ppl_remove['Meaning']))]\n",
    "print(len(full_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To Lower Case\n",
    "full_df = full_df.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows with empty aconym, meaning, or content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_empty(row):\n",
    "    mean = row['Meaning']\n",
    "    acr = row['Acronym']\n",
    "    cont = row['Content']\n",
    "    \n",
    "    if len(cont) < 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df['Remove'] = full_df.apply(check_empty, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410954\n",
      "410915\n"
     ]
    }
   ],
   "source": [
    "# Drop rows\n",
    "print(len(full_df))\n",
    "full_df = full_df[full_df['Remove'] == 0]\n",
    "print(len(full_df))\n",
    "\n",
    "# Delete Remove Column\n",
    "del full_df['Remove']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410915\n",
      "409709\n"
     ]
    }
   ],
   "source": [
    "print(len(full_df))\n",
    "full_df = full_df.drop_duplicates(subset=['Acronym', 'Meaning', 'Content'], keep='first')\n",
    "print(len(full_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rows With Dates\n",
    "These mainly contain people, songs, tv shows, and other non-relevant content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contains_date(content):\n",
    "    try:\n",
    "        dates_str = content[content.find(\"(\")+1:content.find(\")\")]\n",
    "        date_strs = re.split(' â€“ | -', dates_str)\n",
    "        dts = [parser.parse(d) for d in date_strs]\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df['Contains_Date'] = full_df['Content'].apply(contains_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409709\n",
      "376919\n"
     ]
    }
   ],
   "source": [
    "print(len(full_df))\n",
    "full_df = full_df[full_df['Contains_Date'] == False]\n",
    "del full_df['Contains_Date']\n",
    "print(len(full_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove names if content contains (born"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove names if content contains '(born'\n",
    "def remove_names(row):\n",
    "    c = row[\"Content\"]\n",
    "    if \"(born\" in c:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376919\n"
     ]
    }
   ],
   "source": [
    "print(len(full_df))\n",
    "full_df[\"Remove_Born\"] = full_df.apply(remove_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261879\n"
     ]
    }
   ],
   "source": [
    "# As a tangent, might as well save the people as a separate database\n",
    "people = full_df[full_df['Remove_Born'] == True]\n",
    "people.to_pickle('DataSets/people.pkl')\n",
    "\n",
    "full_df = full_df[full_df['Remove_Born'] == False]\n",
    "del full_df['Remove_Born']\n",
    "\n",
    "print(len(full_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4055941\n",
      "901.0211881713005\n",
      "4032756\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_cont(content):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in content.split()])\n",
    "\n",
    "cnt = collections.Counter(' '.join(full_df['Content']).split())\n",
    "\n",
    "print(len(cnt))\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "full_df['Content'] = full_df['Content'].apply(lemmatize_cont)\n",
    "\n",
    "print(timeit.default_timer() - start_time)\n",
    "\n",
    "cnt = collections.Counter(' '.join(full_df['Content']).split())\n",
    "print(len(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.65458959529451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "151304"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_cont(mean):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in mean.split()])\n",
    "\n",
    "cnt = collections.Counter(' '.join(full_df['Meaning']).split())\n",
    "\n",
    "len(cnt)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "full_df['Meaning'] = full_df['Meaning'].apply(lemmatize_cont)\n",
    "\n",
    "print(timeit.default_timer() - start_time)\n",
    "\n",
    "cnt = collections.Counter(' '.join(full_df['Meaning']).split())\n",
    "len(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In Meanings, Replace hyphons with spaces\n",
    "def rep_hyp_with_sp(row):\n",
    "    mean = row['Meaning']\n",
    "    return mean.replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df['Meaning'] = full_df.apply(rep_hyp_with_sp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove special characters from content\n",
    "def remove_special_char(row):\n",
    "    content = row['Content']\n",
    "\n",
    "    return re.sub('[^A-Za-z0-9\\s]+', '', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.98787504476286\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "full_df['Content'] = full_df.apply(remove_special_char, axis=1)\n",
    "\n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove meaningless words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.90124757459944\n"
     ]
    }
   ],
   "source": [
    "ignore = ['1','2','3','4','5','6','7','8','9','0',' ','or','is','the','be','to','of','and','in','that','have','it','for','not','on','with',\n",
    "         'a','an','as','do','at','this','but','by']\n",
    "\n",
    "def remove_meaningless(content):\n",
    "    return ' '.join([w for w in content.split() if w not in ignore])\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "full_df['Content'] = full_df['Content'].apply(remove_meaningless)\n",
    "\n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find names by frequency of seeing his, her, he, she, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person_words = \"him his her he she\".split()\n",
    "\n",
    "def person_in(content):\n",
    "    top = [r[0] for r in collections.Counter(content.split()).most_common()[:10]]\n",
    "    for w in top:\n",
    "        if w in person_words:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "people['Person_In'] = people['Content'].apply(person_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115040"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261879\n",
      "223950\n"
     ]
    }
   ],
   "source": [
    "print(len(full_df))\n",
    "full_df['Person_In'] = full_df['Content'].apply(person_in)\n",
    "\n",
    "people = people.append(full_df[full_df['Person_In'] == True])\n",
    "people.to_pickle('DataSets/people.pkl')\n",
    "\n",
    "full_df = full_df[full_df['Person_In'] == False]\n",
    "del full_df['Person_In']\n",
    "print(len(full_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152969"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find names by comparing meaning against name database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Names\n",
    "f = open(\"DataSets/names.txt\", \"r\")\n",
    "first_names = []\n",
    "for r in f:\n",
    "    first_names += [r.split()[0].lower()]\n",
    "    \n",
    "f = open(\"DataSets/lastnames.txt\", \"r\")\n",
    "last_names = []\n",
    "for r in f:\n",
    "    last_names += [r.split()[0].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def meaning_is_name(mean):\n",
    "    return all([True if w in first_names or w in last_names else False for w in mean.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223950\n",
      "196774\n",
      "1455.2639543919354\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "print(len(full_df))\n",
    "full_df['Person_In'] = full_df['Meaning'].apply(meaning_is_name)\n",
    "\n",
    "people = people.append(full_df[full_df['Person_In'] == True])\n",
    "people.to_pickle('DataSets/people.pkl')\n",
    "\n",
    "full_df = full_df[full_df['Person_In'] == False]\n",
    "del full_df['Person_In']\n",
    "print(len(full_df))\n",
    "\n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180145"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this to find new rows that shouldn't be in the data\n",
    "\n",
    "# idx = randint(0, len(full_df))\n",
    "# print(full_df['Acronym'].iloc[idx])\n",
    "# print(full_df['Meaning'].iloc[idx])\n",
    "# print()\n",
    "# print(full_df['Content'].iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows where each word in Meaning is not common in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_counts = collections.Counter(' '.join(full_df['Content'].values.tolist()).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresh = 0.000015\n",
    "total_cnts = sum(corpus_counts.values())\n",
    "\n",
    "def mean_not_common(mean):\n",
    "    return all([True if corpus_counts[w]/total_cnts < thresh else False for w in mean.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8719645920764378\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "full_df['Not_Common'] = full_df['Meaning'].apply(mean_not_common)\n",
    "full_df = full_df[full_df['Not_Common'] == False]\n",
    "del full_df['Not_Common']\n",
    "\n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153880"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Data\n",
    "full_df.to_pickle('DataSets/before_lemmatize.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_pickle('DataSets/before_lemmatize.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove words from Content that are infrequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1370778\n",
      "15433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cradle', 5.014356516872947e-06),\n",
       " ('nyu', 5.014356516872947e-06),\n",
       " ('attributes', 5.014356516872947e-06),\n",
       " ('corey', 5.014356516872947e-06),\n",
       " ('ideally', 5.014356516872947e-06),\n",
       " ('towing', 5.014356516872947e-06),\n",
       " ('intramural', 5.014356516872947e-06),\n",
       " ('printers', 5.014356516872947e-06),\n",
       " ('monochrome', 5.014356516872947e-06),\n",
       " ('deportation', 5.014356516872947e-06),\n",
       " ('codified', 5.014356516872947e-06),\n",
       " ('economies', 5.014356516872947e-06),\n",
       " ('reunification', 5.014356516872947e-06),\n",
       " ('importing', 5.014356516872947e-06),\n",
       " ('estimates', 5.014356516872947e-06),\n",
       " ('olivier', 5.014356516872947e-06),\n",
       " ('hungry', 5.014356516872947e-06),\n",
       " ('viktor', 5.014356516872947e-06),\n",
       " ('twentyfour', 5.014356516872947e-06),\n",
       " ('confusing', 5.014356516872947e-06)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_counts = collections.Counter(' '.join(full_df['Content'].values.tolist()).split())\n",
    "\n",
    "total_cnts = sum(corpus_counts.values())\n",
    "word_tf = {w: corpus_counts[w]/total_cnts for w in corpus_counts.keys()}\n",
    "filtered_vocab = {w: v for w, v in word_tf.items() if v > 0.000005}\n",
    "print(len(word_tf))\n",
    "print(len(filtered_vocab))\n",
    "collections.Counter(filtered_vocab).most_common()[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67605883"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(' '.join(full_df['Content']).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.309187675103203\n"
     ]
    }
   ],
   "source": [
    "def remove_infrequent(content):\n",
    "    return ' '.join([w for w in content.split() if w in filtered_vocab.keys()])\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "full_df['Content'] = full_df['Content'].apply(remove_infrequent)\n",
    "\n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58388288"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(' '.join(full_df['Content']).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Data\n",
    "full_df.to_pickle('DataSets/custom_clean_acronym_intermediate.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start With Existing Data Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_pickle('DataSets/custom_clean_acronym_intermediate.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows where letters in Acronym are not letters or numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proper_acr = 'a b c d e f g h i j k l m n o p q r s t u v w x y z 1 2 3 4 5 6 7 8 9 0'.split()\n",
    "\n",
    "def remove_by_acr(acr):\n",
    "    for l in acr:\n",
    "        if l not in proper_acr:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153880\n",
      "153342\n"
     ]
    }
   ],
   "source": [
    "print(len(full_df))\n",
    "full_df['Remove'] = full_df['Acronym'].apply(remove_by_acr)\n",
    "full_df = full_df[full_df['Remove'] == False]\n",
    "del full_df['Remove']\n",
    "print(len(full_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = len(full_df)\n",
    "def get_tf(row):\n",
    "    content = row['Content']\n",
    "    counts = collections.Counter(content.split())\n",
    "    \n",
    "    cont_len = len(content.split())\n",
    "    tf_dict = {w:counts[w]/cont_len for w in counts}\n",
    "    \n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.66344976816799\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "full_df['TF'] = full_df.apply(get_tf, axis=1)\n",
    "\n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Movies, Albums, Songs w/ TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_words = 'film movie band song singer album imdb actress actor sitcom university school'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_with_tf(row_tf, rem_words):\n",
    "    tf = collections.Counter(row_tf)\n",
    "    mc = [x[0] for x in tf.most_common()[:10]]\n",
    "    for w in mc:\n",
    "        if w in rem_words:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153342\n",
      "144357\n",
      "24.14344504024575\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "print(len(full_df))\n",
    "full_df['Remove'] = full_df['TF'].apply(remove_with_tf, args=(remove_words,))\n",
    "full_df = full_df[full_df['Remove'] == False]\n",
    "del full_df['Remove']\n",
    "print(len(full_df))\n",
    "\n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Words that occur too often in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words_remove = 50\n",
    "corpus_counts = collections.Counter(' '.join(full_df['Content'].values.tolist()).split())\n",
    "\n",
    "total_cnts = sum(corpus_counts.values())\n",
    "word_tf = {w: corpus_counts[w]/total_cnts for w in corpus_counts.keys()}\n",
    "corpus_tf = collections.Counter(word_tf)\n",
    "remove_words = [w for w, v in corpus_tf.most_common()[:num_words_remove]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These words account for 13.7% of the corpus.\n"
     ]
    }
   ],
   "source": [
    "pct = sum([v for w, v in corpus_tf.most_common()[:num_words_remove]])*100\n",
    "print(\"These words account for \" + '{0:.1f}'.format(pct) + \"% of the corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.26941607821209\n"
     ]
    }
   ],
   "source": [
    "def remove_frequent(content):\n",
    "    return ' '.join([w for w in content.split() if w not in remove_words])\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "full_df['Content'] = full_df['Content'].apply(remove_frequent)\n",
    "\n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 54833169 in the corpus, now there are 47332943\n"
     ]
    }
   ],
   "source": [
    "corpus_counts = collections.Counter(' '.join(full_df['Content'].values.tolist()).split())\n",
    "print('There were ' + str(total_cnts) + ' in the corpus, now there are ' + str(sum(corpus_counts.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "# del full_df['TF']\n",
    "# del full_df['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove acr/meanings from content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove acronyms/meanings from content\n",
    "def rem_meanings_from_content(row):\n",
    "    acr = row['Acronym']\n",
    "    mean = row['Meaning']\n",
    "    content = row['Content']\n",
    "    \n",
    "    for w in mean.split():\n",
    "        content = content.replace(w, '')\n",
    "    \n",
    "    content = content.replace(acr, '')\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df['Content'] = full_df.apply(rem_meanings_from_content, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Data\n",
    "full_df.to_pickle('DataSets/custom_clean_acronym_with_content.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start With Existing Clean Data Here [Acronym, Meaning, Content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_pickle('DataSets/custom_clean_acronym_with_content.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get TFIDF for Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_counts = collections.Counter(' '.join(full_df['Content'].values.tolist()).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TFIDFs for Each Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = len(full_df)\n",
    "def get_tfidf(row):\n",
    "    content = row['Content']\n",
    "    counts = collections.Counter(content.split())\n",
    "    \n",
    "#     content_dict = {}\n",
    "#     for w in counts:\n",
    "#         tf = counts[w]/len(content.split())\n",
    "#         idf = math.log(N/corpus_counts[w])\n",
    "#         content_dict[w] = tf*idf\n",
    "\n",
    "    cont_len = len(content.split())   \n",
    "\n",
    "    # content_dict = {w: tf*idf}  \n",
    "    # tf = counts[w]/cont_len\n",
    "    # idf = math.log(N/corpus_counts[w]) \n",
    "                   \n",
    "    content_dict = {w:(counts[w]/cont_len)*math.log(N/corpus_counts[w]) for w in counts}\n",
    "    \n",
    "    return content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.519573773139314\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "full_df['TFIDF'] = full_df.apply(get_tfidf, axis=1)\n",
    "\n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_tags = 50 # Number of tags for each acronym\n",
    "def get_tags(row):\n",
    "    tfidf_row = row['TFIDF']\n",
    "    tfidf = collections.Counter(tfidf_row)\n",
    "    tags = list(dict(tfidf.most_common()[:num_tags]).keys())\n",
    "    tags = [tag for tag in tags if len(tag) < 15] # Remove rediculously long tags\n",
    "    \n",
    "    min_rand_len = 6\n",
    "    max_rand_len = len(tags)\n",
    "    if len(tags) < min_rand_len:\n",
    "        max_rand_len = min_rand_len\n",
    "    rand_len = randint(min_rand_len, max_rand_len)\n",
    "    tags = tags[:rand_len]\n",
    "    \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['Tags'] = full_df.apply(get_tags, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del full_df['TFIDF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rows if Len of TFIDF is below thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df['Length'] = full_df['TFIDF'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df = full_df[full_df['Length'] >= 5]\n",
    "del full_df['Length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'Vocab To Int'/'Int To Vocab' Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "vocab_to_int = {'<PAD>': 0, '<UNK>': 1, '</ACR>': 2, '<GO>': 3, '</TAG>': 4, '</MEAN>': 5}\n",
    "\n",
    "cnt = collections.Counter(' '.join(full_df['Meaning']).split())\n",
    "cnt.update([t for tags in full_df['Tags'] for t in tags])\n",
    "cnt.update('a b c d e f g h i j k l m n o p q r s t u v w x y z'.split())\n",
    "\n",
    "# Keep vocabulary with counts greater than threshold\n",
    "thresh = 0\n",
    "c = count(len(vocab_to_int))\n",
    "vocab_to_int.update({w: next(c) for (w, val) in cnt.items() if val > thresh}) \n",
    "int_to_vocab = {i: w for w, i in vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113082"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Dictionaries to Pickle files\n",
    "pickle.dump(vocab_to_int, open(\"DataSets/custom_vocab_to_int.p\", \"wb\"))\n",
    "pickle.dump(int_to_vocab, open(\"DataSets/custom_int_to_vocab.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create duplicate rows with different tag sets\n",
    "\n",
    "Example:\n",
    "- acs\taccess control service\t[web, factored, swt, draft, federation]\n",
    "- acs\taccess control service\t[platform, customizable, python, windows, brow...\n",
    "- acs\taccess control service\t[programmatic, portal, integration, google, ac..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-91d39ed93152>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfull_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TFIDF'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_tags\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-91d39ed93152>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfull_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TFIDF'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_tags\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "keys = set(vocab_to_int.keys())\n",
    "\n",
    "num_tags = 50\n",
    "#interval = 10\n",
    "num_tag_sets = 1\n",
    "len_tag_set = 20\n",
    "df_exp = pd.DataFrame(columns=['Acronym', 'Meaning', 'Tags'])\n",
    "\n",
    "sliced_tags = []\n",
    "acronyms = []\n",
    "meanings = []\n",
    "\n",
    "for idx, row in full_df.iterrows():\n",
    "    mc = [r[0] for r in collections.Counter(row['TFIDF']).most_common() if r[0] in keys]\n",
    "    tags = mc[:num_tags]\n",
    "    \n",
    "    \n",
    "    if len(tags) <= 3:\n",
    "        continue\n",
    "    \n",
    "    #trunc_num_tags = (len(tags)//interval)*interval\n",
    "    \n",
    "    #sliced = [tags[i:i+interval] for i in range(0, trunc_num_tags, interval)]\n",
    "    #sliced_tags += sliced\n",
    "    \n",
    "    # Take random values from full num_tags and get a subset of length len_tag_set\n",
    "    # This will create multiple lists of tags with similar tag sets, but not exactly the same    \n",
    "    slice_len = len_tag_set\n",
    "    if slice_len > len(tags):\n",
    "        slice_len = len(tags)\n",
    "        \n",
    "    if len(tags) > 1.5*len_tag_set:  \n",
    "        tag_set_cnt = num_tag_sets\n",
    "        for i in range(num_tag_sets):\n",
    "            sliced_tags += [[tags[i] for i in random.sample(range(len(tags)), slice_len)]]\n",
    "    else:\n",
    "        tag_set_cnt = 1\n",
    "        sliced_tags += [tags]\n",
    "        \n",
    "    #list_len = len(sliced)\n",
    "    \n",
    "    acronyms += [row['Acronym']]*tag_set_cnt\n",
    "    meanings += [row['Meaning']]*tag_set_cnt\n",
    "    \n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_exp = df_exp.append(pd.DataFrame({'Acronym': acronyms,'Meaning': meanings,'Tags': sliced_tags}))\n",
    "full_df = df_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Data\n",
    "full_df.to_pickle('DataSets/custom_acronyms_with_tags.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with Existing Data Here [Acronym, Meaning, Content, Tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('DataSets/custom_acronyms_with_tags.pkl')\n",
    "vocab_to_int = pickle.load(open(\"DataSets/custom_vocab_to_int.p\", \"rb\"))\n",
    "int_to_vocab = pickle.load(open(\"DataSets/custom_int_to_vocab.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143150"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Text To IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data To IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<ACR>', 'a', 'c', 'r', 'o', 'n', 'y', 'm', '</ACR>', '<TAG>', 'these', 'are', 'the', 'tags', '</TAG>']\n",
      "['<MEAN>', 'this', 'is', 'the', 'meaning', '</MEAN>']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Probably don't need <ACR> at the beginning of encoder inputs. Might want to try without.\n",
    "\n",
    "# <ACR>,a,c,r,o,n,y,m,</ACR>,<TAG>,these,are,the,tags</TAG>\n",
    "\n",
    "# Example:\n",
    "a = 'acronym'\n",
    "b = ['these', 'are', 'the', 'tags']\n",
    "c = ('<ACR> ' + ' '.join(list(a)) + ' </ACR> ' + '<TAG> ' + ' '.join(b) + ' </TAG>').split()\n",
    "e = 'this is the meaning'\n",
    "f = ('<MEAN> ' + e + ' </MEAN>').split()\n",
    "print(c)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_acr_len = max([len(a) for a in list(df['Acronym'])])\n",
    "#trunc_thresh = 20 # If There are more than this many tags, than truncate the rest off\n",
    "# keys = set(vocab_to_int.keys())\n",
    "\n",
    "def data_2_ids(row):\n",
    "    acr = row['Acronym']\n",
    "    mean = row['Meaning']\n",
    "    tags = row['Tags']\n",
    "    \n",
    "    # Get a list of tags that occur in the vocab\n",
    "#     valid_tags = []\n",
    "#     for tag in tags:\n",
    "#         if tag in set(vocab_to_int.keys()):\n",
    "#             valid_tags += [tag]\n",
    "#     valid_tags = [tag for tag in tags if tag in keys]\n",
    "    \n",
    "    # Trim tags if there are more than thresh\n",
    "#     if len(valid_tags) > trunc_thresh:\n",
    "#         valid_tags = valid_tags[:trunc_thresh-1]\n",
    "    \n",
    "    # Removed '<ACR> ' and '<TAG>' from beginning of source_text. Don't think I need them.\n",
    "    # Source text shouldn't need a start delimeter and having </ACR> & <TAG> is redundant.\n",
    "    #source_text = '<ACR> ' + ' '.join(list(acr)) + ' </ACR> ' + '<TAG> ' + ' '.join(tags) + ' </TAG>'\n",
    "    #source_text = ' '.join(list(acr)) + ' </ACR> ' + ' '.join(valid_tags) + ' </TAG>'\n",
    "    source_text = list(acr) + ['</ACR>'] + tags + ['</TAG>']\n",
    "\n",
    "    # The decoder needs a <GO> tag to know when to start generating output. It also needs an <EOS>\n",
    "    #     to know when a sentence ends or indicate it as output.\n",
    "    #target_text = mean + ' </MEAN>'\n",
    "    target_text = mean.split() + ['</MEAN>']\n",
    "    \n",
    "#     # Add <PAD> to end of source\n",
    "#     for i in range(len(source_text.split()), max_source + 4):\n",
    "#         source_text += ' <PAD>'\n",
    "    \n",
    "#     # Add <PAD> to end of target meaning so all targets have the same length\n",
    "#     for i in range(len(target_text.split()), max_target + 2):\n",
    "#         target_text += ' <PAD>'\n",
    "    \n",
    "    source_id_text = [vocab_to_int.get(w, vocab_to_int['<UNK>']) for w in source_text if w in vocab_to_int]\n",
    "    target_id_text = [vocab_to_int.get(w, vocab_to_int['<UNK>']) for w in target_text]\n",
    "    \n",
    "    return pd.Series([row['Acronym'], row['Meaning'], row['Tags'], source_id_text, target_id_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.78163758136361\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "df2 = df.apply(data_2_ids, axis=1)\n",
    "df2.columns = ['Acronym', 'Meaning', 'Tags', 'Source', 'Target']\n",
    "\n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df2 = df2[df2['Remove'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Data\n",
    "df2.to_pickle('DataSets/custom_acronyms_with_tags_src_trgt.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start With Existing Data Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle('DataSets/custom_acronyms_with_tags_src_trgt.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unkown Meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove rows where target is all unknowns <UNK>\n",
    "thresh = 0.50\n",
    "\n",
    "def mostly_unk(tar):\n",
    "    unk_cnt = 0\n",
    "    known_cnt = 0\n",
    "    \n",
    "    for w in tar:\n",
    "        if w == vocab_to_int['<UNK>']:\n",
    "            unk_cnt += 1\n",
    "        elif w == vocab_to_int['</MEAN>']:\n",
    "            pass\n",
    "        else:\n",
    "            known_cnt += 1\n",
    "            \n",
    "    total = known_cnt + unk_cnt\n",
    "    pct = unk_cnt/total\n",
    "    \n",
    "    if tar[len(tar)-1] == vocab_to_int['</TAG>'] and tar[len(tar)-2] == vocab_to_int['</ACR>']:\n",
    "        return True\n",
    "    \n",
    "    if pct > thresh:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143150"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2['Remove'] = df2['Target'].apply(mostly_unk)\n",
    "df2 = df2[df2['Remove'] == False]\n",
    "del df2['Remove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2['Remove'] = df2['Source'].apply(mostly_unk)\n",
    "df2 = df2[df2['Remove'] == False]\n",
    "del df2['Remove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143150"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "df2 = shuffle(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_split_percent = 0.9\n",
    "train_split_size = round(len(df2)*train_split_percent)\n",
    "validation_set = df2[train_split_size:]\n",
    "df2 = df2[:train_split_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data set\n",
    "validation_set.to_pickle('DataSets/custom_acr_with_src_trgt_validation.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort By Source Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_len(col):\n",
    "#     return len(col)\n",
    "\n",
    "# df2['Source_Len'] = df2['Source'].apply(get_len)\n",
    "# df2['Target_Len'] = df2['Target'].apply(get_len)\n",
    "# df3 = df2.sort_values(['Source_Len', 'Target_Len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Save data set\n",
    "# df3.to_pickle('DataSets/custom_acr_with_src_trgt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save data set\n",
    "df2.to_pickle('DataSets/custom_acr_with_src_trgt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128835"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Set From Here with Source/Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('DataSets/custom_acr_with_src_trgt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acronym</th>\n",
       "      <th>Meaning</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bjt</td>\n",
       "      <td>bipolar junction transistor</td>\n",
       "      <td>[charge, mode, direction, bias, diffusion, flo...</td>\n",
       "      <td>[57427, 69588, 21816, 2, 25858, 27383, 91557, ...</td>\n",
       "      <td>[73997, 614, 58590, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bjt</td>\n",
       "      <td>bhandasar jain temple</td>\n",
       "      <td>[rajasthan, pillar, gallery, yellowstone, retr...</td>\n",
       "      <td>[57427, 69588, 21816, 2, 68150, 13344, 6852, 6...</td>\n",
       "      <td>[30466, 9828, 505, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bjt</td>\n",
       "      <td>business jet traveler</td>\n",
       "      <td>[editorial, publishers, marketed, countrys, ma...</td>\n",
       "      <td>[57427, 69588, 21816, 2, 75158, 41040, 74842, ...</td>\n",
       "      <td>[66572, 9710, 6189, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bjt</td>\n",
       "      <td>brockley jack theatre</td>\n",
       "      <td>[plus, workshop, play, visiting, venue, regist...</td>\n",
       "      <td>[57427, 69588, 21816, 2, 104354, 77036, 2004, ...</td>\n",
       "      <td>[58221, 18291, 12384, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bjt</td>\n",
       "      <td>belgrade jazz trio</td>\n",
       "      <td>[festival, clarinet, yugoslavia, then, guitar,...</td>\n",
       "      <td>[57427, 69588, 21816, 2, 70767, 19634, 108836,...</td>\n",
       "      <td>[87705, 33823, 26231, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bjt</td>\n",
       "      <td>beijing jiaotong tai</td>\n",
       "      <td>[xm, citys, fm, 80, traffic, am, junction, fif...</td>\n",
       "      <td>[57427, 69588, 21816, 2, 94592, 50379, 7285, 3...</td>\n",
       "      <td>[87184, 42559, 27152, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bjt</td>\n",
       "      <td>breitling jet team</td>\n",
       "      <td>[engagement, 2002, lasting, swiss, rhode, fran...</td>\n",
       "      <td>[57427, 69588, 21816, 2, 108806, 103518, 11131...</td>\n",
       "      <td>[56974, 9710, 20125, 5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Acronym                      Meaning  \\\n",
       "0     bjt  bipolar junction transistor   \n",
       "0     bjt        bhandasar jain temple   \n",
       "0     bjt        business jet traveler   \n",
       "0     bjt        brockley jack theatre   \n",
       "0     bjt           belgrade jazz trio   \n",
       "0     bjt         beijing jiaotong tai   \n",
       "0     bjt           breitling jet team   \n",
       "\n",
       "                                                Tags  \\\n",
       "0  [charge, mode, direction, bias, diffusion, flo...   \n",
       "0  [rajasthan, pillar, gallery, yellowstone, retr...   \n",
       "0  [editorial, publishers, marketed, countrys, ma...   \n",
       "0  [plus, workshop, play, visiting, venue, regist...   \n",
       "0  [festival, clarinet, yugoslavia, then, guitar,...   \n",
       "0  [xm, citys, fm, 80, traffic, am, junction, fif...   \n",
       "0  [engagement, 2002, lasting, swiss, rhode, fran...   \n",
       "\n",
       "                                              Source                    Target  \n",
       "0  [57427, 69588, 21816, 2, 25858, 27383, 91557, ...    [73997, 614, 58590, 5]  \n",
       "0  [57427, 69588, 21816, 2, 68150, 13344, 6852, 6...     [30466, 9828, 505, 5]  \n",
       "0  [57427, 69588, 21816, 2, 75158, 41040, 74842, ...    [66572, 9710, 6189, 5]  \n",
       "0  [57427, 69588, 21816, 2, 104354, 77036, 2004, ...  [58221, 18291, 12384, 5]  \n",
       "0  [57427, 69588, 21816, 2, 70767, 19634, 108836,...  [87705, 33823, 26231, 5]  \n",
       "0  [57427, 69588, 21816, 2, 94592, 50379, 7285, 3...  [87184, 42559, 27152, 5]  \n",
       "0  [57427, 69588, 21816, 2, 108806, 103518, 11131...   [56974, 9710, 20125, 5]  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Acronym'] == 'bjt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ConceptNet Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
    "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "embeddings_index = {}\n",
    "with open('DataSets/numberbatch.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(embeddings_index, open(\"DataSets/embeddings_index.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
