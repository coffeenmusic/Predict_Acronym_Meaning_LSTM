{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to add or try:\n",
    "- Remove tar_vocab_to_int from decoding layer and just pass in the int for vocab_to_int['-PAD-']\n",
    "- In preprocessing, valid_tags should be done before data_2_ids where I create duplicate rows for tag sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source data contains ending delimiters/symbols for the Acronym and the Tag. </ACR> & </TAG>\n",
    "# Target data contains a start delimiter <MEAN> and an end delimiter </MEAN>\n",
    "# The <PAD> padding symbol is added in the network's graph and only padded to the size of the largest row in each current batch.\n",
    "\n",
    "# Example Source. Padding added later.\n",
    "# ['a', 'c', 'r', 'o', 'n', 'y', 'm', '</ACR>', '<TAG>', 'these', 'are', 'the', 'tags', '</TAG>']\n",
    "\n",
    "# Example Target. Padding added later.\n",
    "# ['<MEAN>', 'this', 'is', 'the', 'meaning', '</MEAN>']\n",
    "\n",
    "# *Actual data in indexed integer format for use with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('DataSets/custom_acr_with_src_trgt.pkl')\n",
    "# validation_set = pd.read_pickle('DataSets/custom_acr_with_src_trgt_validation.pkl')\n",
    "vocab_to_int = pickle.load(open(\"DataSets/custom_vocab_to_int.p\", \"rb\"))\n",
    "int_to_vocab = pickle.load(open(\"DataSets/custom_int_to_vocab.p\", \"rb\"))\n",
    "\n",
    "embeddings_index = pickle.load(open(\"DataSets/embeddings_index.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_int(text):\n",
    "    temp = []\n",
    "    for i in text:\n",
    "        temp.append(int_to_vocab[i])\n",
    "    print(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(read_int(df['Target'].iloc[2]), end='\\n\\n')\n",
    "# read_int(df['Source'].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# source_int_text = [r for r in df['Source']]\n",
    "# target_int_text = [r for r in df['Target']]\n",
    "# source_int_text_val = [r for r in validation_set['Source']]\n",
    "# target_int_text_val = [r for r in validation_set['Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    df = pd.read_pickle('DataSets/custom_acr_with_src_trgt.pkl')\n",
    "    validation_set = pd.read_pickle('DataSets/custom_acr_with_src_trgt_validation.pkl')\n",
    "    \n",
    "    src = [r for r in df['Source']] # Source Data in int form\n",
    "    tgt = [r for r in df['Target']] # Target Data in int form\n",
    "    src_val = [r for r in validation_set['Source']] # Validation Set Source Data in int form\n",
    "    tgt_val = [r for r in validation_set['Target']] # Validation Set Target Data in int form\n",
    "    return src, tgt, src_val, tgt_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_int_text, target_int_text, source_int_text_val, target_int_text_val = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not currently used because it is faster to train when sorting by source length\n",
    "\n",
    "# Shuffle data\n",
    "# from random import shuffle\n",
    "# idx_shuffle = [i for i in range(len(source_int_text))]\n",
    "# shuffle(idx_shuffle)\n",
    "# source_int_text = [source_int_text[i] for i in idx_shuffle]\n",
    "# target_int_text = [target_int_text[i] for i in idx_shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word Embeddings From ConceptNet Numberbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ConceptNet Numberbatch uses embedding dimension of 300\n",
    "embedding_dim = 300\n",
    "\n",
    "word_embedding_matrix = np.zeros((len(vocab_to_int), embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target_length = len(df['Target'].iloc[0])\n",
    "# source_length = len(df['Source'].iloc[0])\n",
    "source_length = len(source_int_text[0])\n",
    "target_length = len(target_int_text[0])\n",
    "\n",
    "\n",
    "vocab_size = len(vocab_to_int)\n",
    "\n",
    "pad_int = vocab_to_int['<PAD>'] # <PAD> int value\n",
    "target_start_int = vocab_to_int['<GO>']\n",
    "target_stop_int = vocab_to_int['</MEAN>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, learning rate, and lengths of source and target sequences.\n",
    "    :return: Tuple (input, targets, learning rate, keep probability, target sequence length,\n",
    "    max target sequence length, source sequence length)\n",
    "    \"\"\"\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='target')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_probability')\n",
    "    target_seq_len = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_seq_len, name='max_target_length')\n",
    "    source_seq_len = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "    \n",
    "    return inputs, targets, learning_rate, keep_prob, target_seq_len, max_target_len, source_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, source_seq_len, source_vocab_size):\n",
    "    \"\"\"\n",
    "    Create encoding layer\n",
    "    :param rnn_inputs: Inputs for the RNN\n",
    "    :param rnn_size: RNN Size (number of units in the cell)\n",
    "    :param num_layers: Number of layers\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :param source_sequence_length: a list of the lengths of each sequence in the batch\n",
    "    :param source_vocab_size: vocabulary size of source data\n",
    "    :param encoding_embedding_size: embedding size of source data\n",
    "    :return: tuple (RNN out, RNN state)\n",
    "    \"\"\"\n",
    "    # Embedding\n",
    "    embedded_encoder_input = tf.nn.embedding_lookup(word_embedding_matrix, rnn_inputs)\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob)\n",
    "            \n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size, tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob)\n",
    "            \n",
    "            out, state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, embedded_encoder_input, source_seq_len, dtype=tf.float32)\n",
    "    \n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    out = tf.concat(out, 2)\n",
    "    \n",
    "    return out, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(tar_data, tar_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    :param tar_data: Target Placehoder\n",
    "    :param tar_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    # Slice off last timestep since it will never be used\n",
    "    ending = tf.strided_slice(tar_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], tar_vocab_to_int['<GO>']), ending], 1)\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding is broken in to 2 graphs that share weights, Training and Inference:\n",
    "- Training takes targets as inputs to each time step\n",
    "- Inference uses previous time step output as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Decoder......................................................................................................\n",
    "\n",
    "def decoding_layer_train(enc_state, dec_cell, dec_embed_input, target_seq_len, max_batch_seq_len, output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    :param enc_state: Encoder State\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param target_seq_len: The lengths of each sequence in the target batch\n",
    "    :param max_batch_seq_len: The length of the longest sequence in the batch\n",
    "    :param output_layer: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input, sequence_length=target_seq_len, time_major=False)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=keep_prob)\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, train_helper, enc_state, output_layer)\n",
    "    out, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, impute_finished=True, maximum_iterations=max_batch_seq_len)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inference Decoder.....................................................................................................\n",
    "\n",
    "def decoding_layer_infer(enc_state, dec_cell, dec_embeddings, start_seq_id, end_seq_id, max_tar_seq_len, vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for inference\n",
    "    :param enc_state: Encoder state\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param start_seq_id: GO ID\n",
    "    :param end_seq_id: EOS Id\n",
    "    :param max_tar_seq_len: Maximum length of target sequences\n",
    "    :param vocab_size: Size of decoder/target vocabulary\n",
    "    :param output_layer: Function to apply the output layer\n",
    "    :param batch_size: Batch size\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "    \"\"\"\n",
    "    start_tokens = tf.tile(tf.constant([start_seq_id], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    emb_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, start_tokens, end_seq_id)\n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, emb_helper, enc_state, output_layer=output_layer)\n",
    "    \n",
    "    # Run the data through the RNN nodes. Does unrolling and returns outputs for each time step and final state of hidden layer.\n",
    "    out, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, impute_finished=True, maximum_iterations=max_tar_seq_len)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the decoding layer...............................................................................................\n",
    "\n",
    "def decoding_layer(dec_input, enc_state, enc_output, src_seq_len, tar_seq_len, max_tar_seq_len, rnn_size, num_layers, tar_vocab_to_int, tar_vocab_size, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :param dec_input: Decoder input\n",
    "    :param enc_state: Encoder state\n",
    "    :param tar_seq_len: The lengths of each sequence in the target batch\n",
    "    :param max_target_sequence_length: Maximum length of target sequences\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param tar_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param tar_vocab_size: Size of target vocabulary\n",
    "    :param batch_size: The size of the batch\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    # Embed Decoder Inputs\n",
    "#     dec_embeddings = tf.Variable(tf.random_uniform([tar_vocab_size, dec_emb_size]))\n",
    "#     dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(word_embedding_matrix, dec_input)\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "    \n",
    "    # Projection Layer, turns top hidden states to logit vectors of dimension tar_vocab_size\n",
    "    output_layer = Dense(tar_vocab_size, kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size, enc_output, src_seq_len, normalize=False, name='BahdanauAttention')\n",
    "    cell = tf.contrib.seq2seq.DynamicAttentionWrapper(drop, attn_mech, rnn_size)\n",
    "    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state[0], _zero_state_tensors(rnn_size, batch_size, tf.float32))\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        train_dec_out = decoding_layer_train(initial_state, \n",
    "                                             cell, \n",
    "                                             dec_embed_input, \n",
    "                                             tar_seq_len, \n",
    "                                             max_tar_seq_len, \n",
    "                                             output_layer, \n",
    "                                             keep_prob)\n",
    "        \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_dec_out = decoding_layer_infer(initial_state, \n",
    "                                             cell, \n",
    "                                             word_embedding_matrix, \n",
    "                                             target_start_int, \n",
    "                                             target_stop_int, \n",
    "                                             max_tar_seq_len, \n",
    "                                             tar_vocab_size, \n",
    "                                             output_layer, \n",
    "                                             batch_size, \n",
    "                                             keep_prob)\n",
    "        \n",
    "        return train_dec_out, infer_dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build sequence to sequence model\n",
    "\n",
    "def seq2seq_model(input_data, tar_data, keep_prob, batch_size, src_seq_len, tar_seq_len, max_tar_seq_len, vocab_size, \n",
    "                  rnn_size, num_layers, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence part of the neural network\n",
    "    :param input_data: Input placeholder\n",
    "    :param tar_data: Target placeholder\n",
    "    :param keep_prob: Dropout keep probability placeholder\n",
    "    :param batch_size: Batch Size\n",
    "    :param source_seq_len: Sequence Lengths of source sequences in the batch\n",
    "    :param tar_seq_len: Sequence Lengths of target sequences in the batch\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param dec_emb: Decoder embedding size\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param tar_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    enc_out, enc_state = encoding_layer(input_data, rnn_size, num_layers, keep_prob, source_seq_len, vocab_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(tar_data, vocab_to_int, batch_size)\n",
    "    \n",
    "    train_dec_out, infer_dec_out = decoding_layer(dec_input, enc_state, enc_out, src_seq_len, tar_seq_len, max_tar_seq_len, rnn_size, num_layers, \n",
    "                                                  vocab_to_int, vocab_size, batch_size, keep_prob)\n",
    "    \n",
    "    return train_dec_out, infer_dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 16\n",
    "rnn_size = 256\n",
    "num_layers = 4\n",
    "learning_rate = 0.0005\n",
    "keep_probability = 0.6\n",
    "\n",
    "display_step = 50\n",
    "update_check = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build The Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell object at 0x000002487AD71898>: The input_size parameter is deprecated.\n",
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell object at 0x00000248D2376FD0>: The input_size parameter is deprecated.\n",
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell object at 0x00000248CA270C88>: The input_size parameter is deprecated.\n",
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.LSTMCell object at 0x00000248C3858EF0>: The input_size parameter is deprecated.\n"
     ]
    }
   ],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    # Load the model inputs\n",
    "    input_data, targets, lr, keep_prob, tar_seq_len, max_tar_seq_len, source_seq_len = model_inputs()\n",
    "    \n",
    "    # Create the training and inference logits\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   source_seq_len,\n",
    "                                                   tar_seq_len,\n",
    "                                                   max_tar_seq_len,\n",
    "                                                   vocab_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   vocab_to_int) \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    masks = tf.sequence_mask(tar_seq_len, max_tar_seq_len, dtype=tf.float32, name='masks')\n",
    "    \n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch   50/8052 - Train Accuracy: 0.38541667, Validation Accuracy: 0.34375000, Loss: 7.10332812\n",
      "Epoch   0 Batch  100/8052 - Train Accuracy: 0.37500000, Validation Accuracy: 0.34375000, Loss: 5.60577044\n",
      "Epoch   0 Batch  150/8052 - Train Accuracy: 0.52678571, Validation Accuracy: 0.37500000, Loss: 5.26438894\n",
      "Epoch   0 Batch  200/8052 - Train Accuracy: 0.41071429, Validation Accuracy: 0.34375000, Loss: 5.02235150\n",
      "Epoch   0 Batch  250/8052 - Train Accuracy: 0.41666667, Validation Accuracy: 0.39583333, Loss: 4.86644246\n",
      "Epoch   0 Batch  300/8052 - Train Accuracy: 0.40000000, Validation Accuracy: 0.41666667, Loss: 4.78379058\n",
      "Epoch   0 Batch  350/8052 - Train Accuracy: 0.44791667, Validation Accuracy: 0.47916667, Loss: 4.83075263\n",
      "Epoch   0 Batch  400/8052 - Train Accuracy: 0.48958333, Validation Accuracy: 0.50000000, Loss: 4.77324216\n",
      "Epoch   0 Batch  450/8052 - Train Accuracy: 0.43750000, Validation Accuracy: 0.50000000, Loss: 4.64471961\n",
      "Epoch   0 Batch  500/8052 - Train Accuracy: 0.64375000, Validation Accuracy: 0.48958333, Loss: 4.64732696\n",
      "Epoch   0 Batch  550/8052 - Train Accuracy: 0.52083333, Validation Accuracy: 0.51041667, Loss: 4.62038728\n",
      "Epoch   0 Batch  600/8052 - Train Accuracy: 0.61718750, Validation Accuracy: 0.51041667, Loss: 4.56985625\n",
      "Epoch   0 Batch  650/8052 - Train Accuracy: 0.45833333, Validation Accuracy: 0.50000000, Loss: 4.63483265\n",
      "Epoch   0 Batch  700/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.52083333, Loss: 4.51338334\n",
      "Epoch   0 Batch  750/8052 - Train Accuracy: 0.52083333, Validation Accuracy: 0.53125000, Loss: 4.45949810\n",
      "Epoch   0 Batch  800/8052 - Train Accuracy: 0.67968750, Validation Accuracy: 0.53125000, Loss: 4.53642804\n",
      "Epoch   0 Batch  850/8052 - Train Accuracy: 0.61458333, Validation Accuracy: 0.52083333, Loss: 4.51270563\n",
      "Epoch   0 Batch  900/8052 - Train Accuracy: 0.57500000, Validation Accuracy: 0.52083333, Loss: 4.49763702\n",
      "Epoch   0 Batch  950/8052 - Train Accuracy: 0.43750000, Validation Accuracy: 0.52083333, Loss: 4.52518922\n",
      "Epoch   0 Batch 1000/8052 - Train Accuracy: 0.55208333, Validation Accuracy: 0.53125000, Loss: 4.32400341\n",
      "Average loss for this update: 4.837\n",
      "New Record!\n",
      "Epoch   0 Batch 1050/8052 - Train Accuracy: 0.48750000, Validation Accuracy: 0.52083333, Loss: 4.43025530\n",
      "Epoch   0 Batch 1100/8052 - Train Accuracy: 0.55357143, Validation Accuracy: 0.53125000, Loss: 4.35629225\n",
      "Epoch   0 Batch 1150/8052 - Train Accuracy: 0.51250000, Validation Accuracy: 0.54166667, Loss: 4.51532691\n",
      "Epoch   0 Batch 1200/8052 - Train Accuracy: 0.55208333, Validation Accuracy: 0.53125000, Loss: 4.31972906\n",
      "Epoch   0 Batch 1250/8052 - Train Accuracy: 0.64062500, Validation Accuracy: 0.54166667, Loss: 4.20364282\n",
      "Epoch   0 Batch 1300/8052 - Train Accuracy: 0.57500000, Validation Accuracy: 0.52083333, Loss: 4.38671424\n",
      "Epoch   0 Batch 1350/8052 - Train Accuracy: 0.47500000, Validation Accuracy: 0.54166667, Loss: 4.25415499\n",
      "Epoch   0 Batch 1400/8052 - Train Accuracy: 0.55208333, Validation Accuracy: 0.53125000, Loss: 4.17635962\n",
      "Epoch   0 Batch 1450/8052 - Train Accuracy: 0.63541667, Validation Accuracy: 0.53125000, Loss: 4.08149344\n",
      "Epoch   0 Batch 1500/8052 - Train Accuracy: 0.62500000, Validation Accuracy: 0.56250000, Loss: 4.27818907\n",
      "Epoch   0 Batch 1550/8052 - Train Accuracy: 0.59375000, Validation Accuracy: 0.55208333, Loss: 4.11237177\n",
      "Epoch   0 Batch 1600/8052 - Train Accuracy: 0.52500000, Validation Accuracy: 0.55208333, Loss: 4.18587731\n",
      "Epoch   0 Batch 1650/8052 - Train Accuracy: 0.53750000, Validation Accuracy: 0.54166667, Loss: 4.19528790\n",
      "Epoch   0 Batch 1700/8052 - Train Accuracy: 0.69531250, Validation Accuracy: 0.54166667, Loss: 4.17497716\n",
      "Epoch   0 Batch 1750/8052 - Train Accuracy: 0.48750000, Validation Accuracy: 0.54166667, Loss: 4.05795764\n",
      "Epoch   0 Batch 1800/8052 - Train Accuracy: 0.60714286, Validation Accuracy: 0.53125000, Loss: 4.07269538\n",
      "Epoch   0 Batch 1850/8052 - Train Accuracy: 0.48958333, Validation Accuracy: 0.53125000, Loss: 3.95059434\n",
      "Epoch   0 Batch 1900/8052 - Train Accuracy: 0.55208333, Validation Accuracy: 0.55208333, Loss: 3.87556386\n",
      "Epoch   0 Batch 1950/8052 - Train Accuracy: 0.57500000, Validation Accuracy: 0.55208333, Loss: 4.05155773\n",
      "Epoch   0 Batch 2000/8052 - Train Accuracy: 0.52083333, Validation Accuracy: 0.57291667, Loss: 4.00081493\n",
      "Average loss for this update: 4.184\n",
      "New Record!\n",
      "Epoch   0 Batch 2050/8052 - Train Accuracy: 0.61458333, Validation Accuracy: 0.53125000, Loss: 4.02714260\n",
      "Epoch   0 Batch 2100/8052 - Train Accuracy: 0.58333333, Validation Accuracy: 0.54166667, Loss: 4.12948986\n",
      "Epoch   0 Batch 2150/8052 - Train Accuracy: 0.60714286, Validation Accuracy: 0.54166667, Loss: 3.84233540\n",
      "Epoch   0 Batch 2200/8052 - Train Accuracy: 0.60416667, Validation Accuracy: 0.58333333, Loss: 3.84787344\n",
      "Epoch   0 Batch 2250/8052 - Train Accuracy: 0.55000000, Validation Accuracy: 0.56250000, Loss: 3.78432895\n",
      "Epoch   0 Batch 2300/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.56250000, Loss: 3.76941909\n",
      "Epoch   0 Batch 2350/8052 - Train Accuracy: 0.69444444, Validation Accuracy: 0.58333333, Loss: 3.82919763\n",
      "Epoch   0 Batch 2400/8052 - Train Accuracy: 0.48750000, Validation Accuracy: 0.54166667, Loss: 3.76067754\n",
      "Epoch   0 Batch 2450/8052 - Train Accuracy: 0.54166667, Validation Accuracy: 0.59375000, Loss: 3.80364193\n",
      "Epoch   0 Batch 2500/8052 - Train Accuracy: 0.48750000, Validation Accuracy: 0.58333333, Loss: 3.80583458\n",
      "Epoch   0 Batch 2550/8052 - Train Accuracy: 0.60416667, Validation Accuracy: 0.58333333, Loss: 3.73459533\n",
      "Epoch   0 Batch 2600/8052 - Train Accuracy: 0.59375000, Validation Accuracy: 0.54166667, Loss: 3.82504323\n",
      "Epoch   0 Batch 2650/8052 - Train Accuracy: 0.65625000, Validation Accuracy: 0.54166667, Loss: 3.79930367\n",
      "Epoch   0 Batch 2700/8052 - Train Accuracy: 0.52083333, Validation Accuracy: 0.56250000, Loss: 3.66233458\n",
      "Epoch   0 Batch 2750/8052 - Train Accuracy: 0.52083333, Validation Accuracy: 0.56250000, Loss: 3.72115689\n",
      "Epoch   0 Batch 2800/8052 - Train Accuracy: 0.55208333, Validation Accuracy: 0.55208333, Loss: 3.84219297\n",
      "Epoch   0 Batch 2850/8052 - Train Accuracy: 0.57291667, Validation Accuracy: 0.58333333, Loss: 3.66599211\n",
      "Epoch   0 Batch 2900/8052 - Train Accuracy: 0.63392857, Validation Accuracy: 0.54166667, Loss: 3.70832914\n",
      "Epoch   0 Batch 2950/8052 - Train Accuracy: 0.63541667, Validation Accuracy: 0.56250000, Loss: 3.69426979\n",
      "Epoch   0 Batch 3000/8052 - Train Accuracy: 0.62500000, Validation Accuracy: 0.54166667, Loss: 3.75738443\n",
      "Average loss for this update: 3.801\n",
      "New Record!\n",
      "Epoch   0 Batch 3050/8052 - Train Accuracy: 0.61458333, Validation Accuracy: 0.55208333, Loss: 3.68558565\n",
      "Epoch   0 Batch 3100/8052 - Train Accuracy: 0.57142857, Validation Accuracy: 0.57291667, Loss: 3.83834546\n",
      "Epoch   0 Batch 3150/8052 - Train Accuracy: 0.60416667, Validation Accuracy: 0.56250000, Loss: 3.72692689\n",
      "Epoch   0 Batch 3200/8052 - Train Accuracy: 0.64583333, Validation Accuracy: 0.55208333, Loss: 3.67662877\n",
      "Epoch   0 Batch 3250/8052 - Train Accuracy: 0.47500000, Validation Accuracy: 0.55208333, Loss: 3.74066580\n",
      "Epoch   0 Batch 3300/8052 - Train Accuracy: 0.51250000, Validation Accuracy: 0.57291667, Loss: 3.67646842\n",
      "Epoch   0 Batch 3350/8052 - Train Accuracy: 0.70312500, Validation Accuracy: 0.53125000, Loss: 3.49093691\n",
      "Epoch   0 Batch 3400/8052 - Train Accuracy: 0.63392857, Validation Accuracy: 0.59375000, Loss: 3.59202625\n",
      "Epoch   0 Batch 3450/8052 - Train Accuracy: 0.58928571, Validation Accuracy: 0.58333333, Loss: 3.55514507\n",
      "Epoch   0 Batch 3500/8052 - Train Accuracy: 0.45000000, Validation Accuracy: 0.57291667, Loss: 3.72135170\n",
      "Epoch   0 Batch 3550/8052 - Train Accuracy: 0.51250000, Validation Accuracy: 0.58333333, Loss: 3.59969213\n",
      "Epoch   0 Batch 3600/8052 - Train Accuracy: 0.50000000, Validation Accuracy: 0.55208333, Loss: 3.60591897\n",
      "Epoch   0 Batch 3650/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.58333333, Loss: 3.57344350\n",
      "Epoch   0 Batch 3700/8052 - Train Accuracy: 0.53571429, Validation Accuracy: 0.58333333, Loss: 3.65460855\n",
      "Epoch   0 Batch 3750/8052 - Train Accuracy: 0.52500000, Validation Accuracy: 0.55208333, Loss: 3.60271750\n",
      "Epoch   0 Batch 3800/8052 - Train Accuracy: 0.52500000, Validation Accuracy: 0.56250000, Loss: 3.57266397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 3850/8052 - Train Accuracy: 0.48750000, Validation Accuracy: 0.53125000, Loss: 3.60844162\n",
      "Epoch   0 Batch 3900/8052 - Train Accuracy: 0.59375000, Validation Accuracy: 0.57291667, Loss: 3.62539948\n",
      "Epoch   0 Batch 3950/8052 - Train Accuracy: 0.59375000, Validation Accuracy: 0.57291667, Loss: 3.74106905\n",
      "Epoch   0 Batch 4000/8052 - Train Accuracy: 0.52500000, Validation Accuracy: 0.60416667, Loss: 3.56265208\n",
      "Average loss for this update: 3.643\n",
      "New Record!\n",
      "Epoch   0 Batch 4050/8052 - Train Accuracy: 0.58750000, Validation Accuracy: 0.60416667, Loss: 3.54360434\n",
      "Epoch   0 Batch 4100/8052 - Train Accuracy: 0.52500000, Validation Accuracy: 0.58333333, Loss: 3.46649690\n",
      "Epoch   0 Batch 4150/8052 - Train Accuracy: 0.60416667, Validation Accuracy: 0.57291667, Loss: 3.61316901\n",
      "Epoch   0 Batch 4200/8052 - Train Accuracy: 0.57500000, Validation Accuracy: 0.57291667, Loss: 3.66052074\n",
      "Epoch   0 Batch 4250/8052 - Train Accuracy: 0.63392857, Validation Accuracy: 0.56250000, Loss: 3.53502894\n",
      "Epoch   0 Batch 4300/8052 - Train Accuracy: 0.57500000, Validation Accuracy: 0.58333333, Loss: 3.68760243\n",
      "Epoch   0 Batch 4350/8052 - Train Accuracy: 0.57500000, Validation Accuracy: 0.57291667, Loss: 3.59326086\n",
      "Epoch   0 Batch 4400/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.54166667, Loss: 3.50537651\n",
      "Epoch   0 Batch 4450/8052 - Train Accuracy: 0.65625000, Validation Accuracy: 0.55208333, Loss: 3.43100173\n",
      "Epoch   0 Batch 4500/8052 - Train Accuracy: 0.62500000, Validation Accuracy: 0.55208333, Loss: 3.64737114\n",
      "Epoch   0 Batch 4550/8052 - Train Accuracy: 0.58928571, Validation Accuracy: 0.55208333, Loss: 3.56683677\n",
      "Epoch   0 Batch 4600/8052 - Train Accuracy: 0.50000000, Validation Accuracy: 0.58333333, Loss: 3.57178293\n",
      "Epoch   0 Batch 4650/8052 - Train Accuracy: 0.46250000, Validation Accuracy: 0.59375000, Loss: 3.50327919\n",
      "Epoch   0 Batch 4700/8052 - Train Accuracy: 0.61458333, Validation Accuracy: 0.55208333, Loss: 3.50708449\n",
      "Epoch   0 Batch 4750/8052 - Train Accuracy: 0.57291667, Validation Accuracy: 0.56250000, Loss: 3.59916935\n",
      "Epoch   0 Batch 4800/8052 - Train Accuracy: 0.48750000, Validation Accuracy: 0.59375000, Loss: 3.52108798\n",
      "Epoch   0 Batch 4850/8052 - Train Accuracy: 0.66071429, Validation Accuracy: 0.56250000, Loss: 3.60969603\n",
      "Epoch   0 Batch 4900/8052 - Train Accuracy: 0.64285714, Validation Accuracy: 0.59375000, Loss: 3.64701955\n",
      "Epoch   0 Batch 4950/8052 - Train Accuracy: 0.58750000, Validation Accuracy: 0.57291667, Loss: 3.57590074\n",
      "Epoch   0 Batch 5000/8052 - Train Accuracy: 0.55000000, Validation Accuracy: 0.58333333, Loss: 3.50441238\n",
      "Average loss for this update: 3.564\n",
      "New Record!\n",
      "Epoch   0 Batch 5050/8052 - Train Accuracy: 0.42187500, Validation Accuracy: 0.57291667, Loss: 3.61426260\n",
      "Epoch   0 Batch 5100/8052 - Train Accuracy: 0.58750000, Validation Accuracy: 0.56250000, Loss: 3.40675931\n",
      "Epoch   0 Batch 5150/8052 - Train Accuracy: 0.58333333, Validation Accuracy: 0.58333333, Loss: 3.63214370\n",
      "Epoch   0 Batch 5200/8052 - Train Accuracy: 0.61607143, Validation Accuracy: 0.59375000, Loss: 3.45093907\n",
      "Epoch   0 Batch 5250/8052 - Train Accuracy: 0.55000000, Validation Accuracy: 0.54166667, Loss: 3.53726894\n",
      "Epoch   0 Batch 5300/8052 - Train Accuracy: 0.45312500, Validation Accuracy: 0.57291667, Loss: 3.57727599\n",
      "Epoch   0 Batch 5350/8052 - Train Accuracy: 0.58750000, Validation Accuracy: 0.56250000, Loss: 3.60484611\n",
      "Epoch   0 Batch 5400/8052 - Train Accuracy: 0.64285714, Validation Accuracy: 0.57291667, Loss: 3.41019049\n",
      "Epoch   0 Batch 5450/8052 - Train Accuracy: 0.62500000, Validation Accuracy: 0.55208333, Loss: 3.59254102\n",
      "Epoch   0 Batch 5500/8052 - Train Accuracy: 0.58333333, Validation Accuracy: 0.57291667, Loss: 3.51536690\n",
      "Epoch   0 Batch 5550/8052 - Train Accuracy: 0.64583333, Validation Accuracy: 0.54166667, Loss: 3.46221000\n",
      "Epoch   0 Batch 5600/8052 - Train Accuracy: 0.74305556, Validation Accuracy: 0.59375000, Loss: 3.48160467\n",
      "Epoch   0 Batch 5650/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.57291667, Loss: 3.50996053\n",
      "Epoch   0 Batch 5700/8052 - Train Accuracy: 0.51250000, Validation Accuracy: 0.56250000, Loss: 3.55730055\n",
      "Epoch   0 Batch 5750/8052 - Train Accuracy: 0.51250000, Validation Accuracy: 0.56250000, Loss: 3.49313635\n",
      "Epoch   0 Batch 5800/8052 - Train Accuracy: 0.67857143, Validation Accuracy: 0.59375000, Loss: 3.47303865\n",
      "Epoch   0 Batch 5850/8052 - Train Accuracy: 0.67187500, Validation Accuracy: 0.58333333, Loss: 3.50025122\n",
      "Epoch   0 Batch 5900/8052 - Train Accuracy: 0.51250000, Validation Accuracy: 0.57291667, Loss: 3.53870476\n",
      "Epoch   0 Batch 5950/8052 - Train Accuracy: 0.55000000, Validation Accuracy: 0.58333333, Loss: 3.41163601\n",
      "Epoch   0 Batch 6000/8052 - Train Accuracy: 0.60000000, Validation Accuracy: 0.59375000, Loss: 3.45116424\n",
      "Average loss for this update: 3.511\n",
      "New Record!\n",
      "Epoch   0 Batch 6050/8052 - Train Accuracy: 0.72916667, Validation Accuracy: 0.58333333, Loss: 3.45304383\n",
      "Epoch   0 Batch 6100/8052 - Train Accuracy: 0.57500000, Validation Accuracy: 0.58333333, Loss: 3.41499882\n",
      "Epoch   0 Batch 6150/8052 - Train Accuracy: 0.65625000, Validation Accuracy: 0.58333333, Loss: 3.49117272\n",
      "Epoch   0 Batch 6200/8052 - Train Accuracy: 0.63541667, Validation Accuracy: 0.58333333, Loss: 3.54325668\n",
      "Epoch   0 Batch 6250/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.57291667, Loss: 3.57827737\n",
      "Epoch   0 Batch 6300/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.58333333, Loss: 3.36319250\n",
      "Epoch   0 Batch 6350/8052 - Train Accuracy: 0.48750000, Validation Accuracy: 0.58333333, Loss: 3.51822198\n",
      "Epoch   0 Batch 6400/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.58333333, Loss: 3.50445899\n",
      "Epoch   0 Batch 6450/8052 - Train Accuracy: 0.52500000, Validation Accuracy: 0.57291667, Loss: 3.32905107\n",
      "Epoch   0 Batch 6500/8052 - Train Accuracy: 0.53750000, Validation Accuracy: 0.58333333, Loss: 3.55157160\n",
      "Epoch   0 Batch 6550/8052 - Train Accuracy: 0.61250000, Validation Accuracy: 0.56250000, Loss: 3.37357209\n",
      "Epoch   0 Batch 6600/8052 - Train Accuracy: 0.50000000, Validation Accuracy: 0.56250000, Loss: 3.47970043\n",
      "Epoch   0 Batch 6650/8052 - Train Accuracy: 0.55000000, Validation Accuracy: 0.59375000, Loss: 3.49247397\n",
      "Epoch   0 Batch 6700/8052 - Train Accuracy: 0.60416667, Validation Accuracy: 0.58333333, Loss: 3.43457405\n",
      "Epoch   0 Batch 6750/8052 - Train Accuracy: 0.52500000, Validation Accuracy: 0.59375000, Loss: 3.41565725\n",
      "Epoch   0 Batch 6800/8052 - Train Accuracy: 0.62500000, Validation Accuracy: 0.57291667, Loss: 3.37407661\n",
      "Epoch   0 Batch 6850/8052 - Train Accuracy: 0.52500000, Validation Accuracy: 0.56250000, Loss: 3.45789180\n",
      "Epoch   0 Batch 6900/8052 - Train Accuracy: 0.63392857, Validation Accuracy: 0.58333333, Loss: 3.37661321\n",
      "Epoch   0 Batch 6950/8052 - Train Accuracy: 0.61458333, Validation Accuracy: 0.58333333, Loss: 3.41831357\n",
      "Epoch   0 Batch 7000/8052 - Train Accuracy: 0.63541667, Validation Accuracy: 0.56250000, Loss: 3.41646764\n",
      "Average loss for this update: 3.449\n",
      "New Record!\n",
      "Epoch   0 Batch 7050/8052 - Train Accuracy: 0.67857143, Validation Accuracy: 0.60416667, Loss: 3.41272175\n",
      "Epoch   0 Batch 7100/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.59375000, Loss: 3.50973213\n",
      "Epoch   0 Batch 7150/8052 - Train Accuracy: 0.57291667, Validation Accuracy: 0.58333333, Loss: 3.33340968\n",
      "Epoch   0 Batch 7200/8052 - Train Accuracy: 0.65178571, Validation Accuracy: 0.60416667, Loss: 3.37955589\n",
      "Epoch   0 Batch 7250/8052 - Train Accuracy: 0.66406250, Validation Accuracy: 0.58333333, Loss: 3.47918206\n",
      "Epoch   0 Batch 7300/8052 - Train Accuracy: 0.64285714, Validation Accuracy: 0.58333333, Loss: 3.51117446\n",
      "Epoch   0 Batch 7350/8052 - Train Accuracy: 0.57500000, Validation Accuracy: 0.57291667, Loss: 3.37493014\n",
      "Epoch   0 Batch 7400/8052 - Train Accuracy: 0.67187500, Validation Accuracy: 0.56250000, Loss: 3.45776461\n",
      "Epoch   0 Batch 7450/8052 - Train Accuracy: 0.54166667, Validation Accuracy: 0.58333333, Loss: 3.38893880\n",
      "Epoch   0 Batch 7500/8052 - Train Accuracy: 0.60000000, Validation Accuracy: 0.56250000, Loss: 3.36237237\n",
      "Epoch   0 Batch 7550/8052 - Train Accuracy: 0.65625000, Validation Accuracy: 0.57291667, Loss: 3.35443360\n",
      "Epoch   0 Batch 7600/8052 - Train Accuracy: 0.50000000, Validation Accuracy: 0.59375000, Loss: 3.56318608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 7650/8052 - Train Accuracy: 0.57291667, Validation Accuracy: 0.57291667, Loss: 3.41058762\n",
      "Epoch   0 Batch 7700/8052 - Train Accuracy: 0.64583333, Validation Accuracy: 0.56250000, Loss: 3.34725412\n",
      "Epoch   0 Batch 7750/8052 - Train Accuracy: 0.55208333, Validation Accuracy: 0.57291667, Loss: 3.40646069\n",
      "Epoch   0 Batch 7800/8052 - Train Accuracy: 0.59375000, Validation Accuracy: 0.59375000, Loss: 3.47576907\n",
      "Epoch   0 Batch 7850/8052 - Train Accuracy: 0.58750000, Validation Accuracy: 0.59375000, Loss: 3.46912821\n",
      "Epoch   0 Batch 7900/8052 - Train Accuracy: 0.51041667, Validation Accuracy: 0.59375000, Loss: 3.44072938\n",
      "Epoch   0 Batch 7950/8052 - Train Accuracy: 0.57500000, Validation Accuracy: 0.60416667, Loss: 3.30074394\n",
      "Epoch   0 Batch 8000/8052 - Train Accuracy: 0.61458333, Validation Accuracy: 0.58333333, Loss: 3.41941498\n",
      "Average loss for this update: 3.42\n",
      "New Record!\n",
      "Epoch   0 Batch 8050/8052 - Train Accuracy: 0.59375000, Validation Accuracy: 0.59375000, Loss: 3.39045576\n",
      "Epoch   1 Batch   50/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.58333333, Loss: 3.15070617\n",
      "Epoch   1 Batch  100/8052 - Train Accuracy: 0.57291667, Validation Accuracy: 0.57291667, Loss: 3.03936630\n",
      "Epoch   1 Batch  150/8052 - Train Accuracy: 0.66071429, Validation Accuracy: 0.59375000, Loss: 3.11357843\n",
      "Epoch   1 Batch  200/8052 - Train Accuracy: 0.60714286, Validation Accuracy: 0.59375000, Loss: 3.05714855\n",
      "Epoch   1 Batch  250/8052 - Train Accuracy: 0.58333333, Validation Accuracy: 0.60416667, Loss: 3.03635282\n",
      "Epoch   1 Batch  300/8052 - Train Accuracy: 0.53750000, Validation Accuracy: 0.60416667, Loss: 3.00453534\n",
      "Epoch   1 Batch  350/8052 - Train Accuracy: 0.52083333, Validation Accuracy: 0.58333333, Loss: 3.08922819\n",
      "Epoch   1 Batch  400/8052 - Train Accuracy: 0.59375000, Validation Accuracy: 0.58333333, Loss: 3.10597559\n",
      "Epoch   1 Batch  450/8052 - Train Accuracy: 0.52500000, Validation Accuracy: 0.57291667, Loss: 3.07918465\n",
      "Epoch   1 Batch  500/8052 - Train Accuracy: 0.70000000, Validation Accuracy: 0.58333333, Loss: 3.08854679\n",
      "Epoch   1 Batch  550/8052 - Train Accuracy: 0.58333333, Validation Accuracy: 0.58333333, Loss: 3.09988419\n",
      "Epoch   1 Batch  600/8052 - Train Accuracy: 0.64062500, Validation Accuracy: 0.59375000, Loss: 3.06956039\n",
      "Epoch   1 Batch  650/8052 - Train Accuracy: 0.61458333, Validation Accuracy: 0.59375000, Loss: 3.12605182\n",
      "Epoch   1 Batch  700/8052 - Train Accuracy: 0.64285714, Validation Accuracy: 0.60416667, Loss: 3.07082259\n",
      "Epoch   1 Batch  750/8052 - Train Accuracy: 0.54166667, Validation Accuracy: 0.60416667, Loss: 3.03946572\n",
      "Epoch   1 Batch  800/8052 - Train Accuracy: 0.67968750, Validation Accuracy: 0.60416667, Loss: 3.08284116\n",
      "Epoch   1 Batch  850/8052 - Train Accuracy: 0.69791667, Validation Accuracy: 0.60416667, Loss: 3.11381189\n",
      "Epoch   1 Batch  900/8052 - Train Accuracy: 0.58750000, Validation Accuracy: 0.59375000, Loss: 3.10653591\n",
      "Epoch   1 Batch  950/8052 - Train Accuracy: 0.45312500, Validation Accuracy: 0.60416667, Loss: 3.15717172\n",
      "Epoch   1 Batch 1000/8052 - Train Accuracy: 0.59375000, Validation Accuracy: 0.58333333, Loss: 3.01732035\n",
      "Average loss for this update: 3.082\n",
      "New Record!\n",
      "Epoch   1 Batch 1050/8052 - Train Accuracy: 0.50000000, Validation Accuracy: 0.59375000, Loss: 3.11168310\n",
      "Epoch   1 Batch 1100/8052 - Train Accuracy: 0.64285714, Validation Accuracy: 0.58333333, Loss: 3.09949329\n",
      "Epoch   1 Batch 1150/8052 - Train Accuracy: 0.55000000, Validation Accuracy: 0.57291667, Loss: 3.20115056\n",
      "Epoch   1 Batch 1200/8052 - Train Accuracy: 0.57291667, Validation Accuracy: 0.59375000, Loss: 3.12450570\n",
      "Epoch   1 Batch 1250/8052 - Train Accuracy: 0.63281250, Validation Accuracy: 0.59375000, Loss: 3.05021914\n",
      "Epoch   1 Batch 1300/8052 - Train Accuracy: 0.58750000, Validation Accuracy: 0.58333333, Loss: 3.19758900\n",
      "Epoch   1 Batch 1350/8052 - Train Accuracy: 0.58750000, Validation Accuracy: 0.61458333, Loss: 3.08210412\n",
      "Epoch   1 Batch 1400/8052 - Train Accuracy: 0.66666667, Validation Accuracy: 0.59375000, Loss: 3.06205064\n",
      "Epoch   1 Batch 1450/8052 - Train Accuracy: 0.67708333, Validation Accuracy: 0.58333333, Loss: 3.01217237\n",
      "Epoch   1 Batch 1500/8052 - Train Accuracy: 0.61458333, Validation Accuracy: 0.58333333, Loss: 3.20289041\n",
      "Epoch   1 Batch 1550/8052 - Train Accuracy: 0.63541667, Validation Accuracy: 0.59375000, Loss: 3.07429451\n",
      "Epoch   1 Batch 1600/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.61458333, Loss: 3.16627498\n",
      "Epoch   1 Batch 1650/8052 - Train Accuracy: 0.60000000, Validation Accuracy: 0.59375000, Loss: 3.19457376\n",
      "Epoch   1 Batch 1700/8052 - Train Accuracy: 0.70312500, Validation Accuracy: 0.59375000, Loss: 3.16771790\n",
      "Epoch   1 Batch 1750/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.60416667, Loss: 3.12069551\n",
      "Epoch   1 Batch 1800/8052 - Train Accuracy: 0.61607143, Validation Accuracy: 0.59375000, Loss: 3.14158823\n",
      "Epoch   1 Batch 1850/8052 - Train Accuracy: 0.53125000, Validation Accuracy: 0.60416667, Loss: 3.09209072\n",
      "Epoch   1 Batch 1900/8052 - Train Accuracy: 0.59375000, Validation Accuracy: 0.60416667, Loss: 3.01980525\n",
      "Epoch   1 Batch 1950/8052 - Train Accuracy: 0.58750000, Validation Accuracy: 0.57291667, Loss: 3.20774574\n",
      "Epoch   1 Batch 2000/8052 - Train Accuracy: 0.56250000, Validation Accuracy: 0.60416667, Loss: 3.14099911\n",
      "Average loss for this update: 3.123\n",
      "No Improvement for 0 Counts\n",
      "Epoch   1 Batch 2050/8052 - Train Accuracy: 0.62500000, Validation Accuracy: 0.58333333, Loss: 3.16255192\n",
      "Epoch   1 Batch 2100/8052 - Train Accuracy: 0.64583333, Validation Accuracy: 0.60416667, Loss: 3.26718965\n",
      "Epoch   1 Batch 2150/8052 - Train Accuracy: 0.63392857, Validation Accuracy: 0.59375000, Loss: 3.08095236\n",
      "Epoch   1 Batch 2200/8052 - Train Accuracy: 0.62500000, Validation Accuracy: 0.58333333, Loss: 3.08025437\n",
      "Epoch   1 Batch 2250/8052 - Train Accuracy: 0.53750000, Validation Accuracy: 0.59375000, Loss: 3.05235970\n",
      "Epoch   1 Batch 2300/8052 - Train Accuracy: 0.58750000, Validation Accuracy: 0.60416667, Loss: 3.02445336\n",
      "Epoch   1 Batch 2350/8052 - Train Accuracy: 0.70138889, Validation Accuracy: 0.61458333, Loss: 3.08273645\n",
      "Epoch   1 Batch 2400/8052 - Train Accuracy: 0.58750000, Validation Accuracy: 0.57291667, Loss: 3.02895133\n",
      "Epoch   1 Batch 2450/8052 - Train Accuracy: 0.60416667, Validation Accuracy: 0.60416667, Loss: 3.07050666\n",
      "Epoch   1 Batch 2500/8052 - Train Accuracy: 0.53750000, Validation Accuracy: 0.60416667, Loss: 3.10907002\n",
      "Epoch   1 Batch 2550/8052 - Train Accuracy: 0.60416667, Validation Accuracy: 0.60416667, Loss: 3.07270308\n",
      "Epoch   1 Batch 2600/8052 - Train Accuracy: 0.59375000, Validation Accuracy: 0.59375000, Loss: 3.15889927\n",
      "Epoch   1 Batch 2650/8052 - Train Accuracy: 0.63541667, Validation Accuracy: 0.59375000, Loss: 3.11106849\n",
      "Epoch   1 Batch 2700/8052 - Train Accuracy: 0.52083333, Validation Accuracy: 0.59375000, Loss: 3.02056484\n",
      "Epoch   1 Batch 2750/8052 - Train Accuracy: 0.53125000, Validation Accuracy: 0.58333333, Loss: 3.05922760\n",
      "Epoch   1 Batch 2800/8052 - Train Accuracy: 0.58333333, Validation Accuracy: 0.60416667, Loss: 3.17969729\n",
      "Epoch   1 Batch 2850/8052 - Train Accuracy: 0.58333333, Validation Accuracy: 0.60416667, Loss: 3.04653427\n",
      "Epoch   1 Batch 2900/8052 - Train Accuracy: 0.65178571, Validation Accuracy: 0.59375000, Loss: 3.05169271\n",
      "Epoch   1 Batch 2950/8052 - Train Accuracy: 0.67708333, Validation Accuracy: 0.60416667, Loss: 3.09302915\n",
      "Epoch   1 Batch 3000/8052 - Train Accuracy: 0.64583333, Validation Accuracy: 0.57291667, Loss: 3.14628654\n",
      "Average loss for this update: 3.095\n",
      "No Improvement for 1 Counts\n",
      "Epoch   1 Batch 3050/8052 - Train Accuracy: 0.62500000, Validation Accuracy: 0.59375000, Loss: 3.08335927\n",
      "Epoch   1 Batch 3100/8052 - Train Accuracy: 0.59821429, Validation Accuracy: 0.60416667, Loss: 3.25564488\n",
      "Epoch   1 Batch 3150/8052 - Train Accuracy: 0.63541667, Validation Accuracy: 0.60416667, Loss: 3.13168393\n",
      "Epoch   1 Batch 3200/8052 - Train Accuracy: 0.66666667, Validation Accuracy: 0.60416667, Loss: 3.09766697\n",
      "Epoch   1 Batch 3250/8052 - Train Accuracy: 0.53750000, Validation Accuracy: 0.60416667, Loss: 3.15522836\n"
     ]
    }
   ],
   "source": [
    "# Split data to training and validation sets\n",
    "# train_source = source_int_text[:train_split_size]\n",
    "# train_target = target_int_text[:train_split_size]\n",
    "# valid_source = source_int_text[train_split_size:]\n",
    "# valid_target = target_int_text[train_split_size:]\n",
    "train_source = source_int_text\n",
    "train_target = target_int_text\n",
    "valid_source = source_int_text_val\n",
    "valid_target = target_int_text_val\n",
    "(valid_sources_batch, \n",
    " valid_targets_batch, \n",
    " valid_sources_lengths, \n",
    " valid_targets_lengths) = next(get_batches(valid_source, \n",
    "                                           valid_target, \n",
    "                                           batch_size, \n",
    "                                           pad_int, \n",
    "                                           pad_int))\n",
    "\n",
    "stop_early = 0\n",
    "update_loss = 0\n",
    "batch_loss = 0\n",
    "batch_total = 0\n",
    "summary_update_loss = []\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    #saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #saver.restore(sess, save_path)\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "            get_batches(train_source, train_target, batch_size, pad_int, pad_int)):\n",
    "        \n",
    "            _, loss = sess.run([train_op, cost], {input_data: source_batch, \n",
    "                                                  targets: target_batch, \n",
    "                                                  lr: learning_rate, \n",
    "                                                  tar_seq_len: targets_lengths, \n",
    "                                                  source_seq_len: sources_lengths, \n",
    "                                                  keep_prob: keep_probability})\n",
    "        \n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "        \n",
    "            # Display Step\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(inference_logits, {input_data: source_batch, \n",
    "                                                                 source_seq_len: sources_lengths, \n",
    "                                                                 tar_seq_len: targets_lengths, \n",
    "                                                                 keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(inference_logits, {input_data: valid_sources_batch, \n",
    "                                                                 source_seq_len: valid_sources_lengths, \n",
    "                                                                 tar_seq_len: valid_targets_lengths,\n",
    "                                                                 keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "                \n",
    "                \n",
    "                \n",
    "                #print('Sources Lengths: {}, Targets Lengths: {}'.format(max(sources_lengths), max(targets_lengths)))\n",
    "                \n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.8f}, Validation Accuracy: {:>6.8f}, Loss: {:>6.8f}'\n",
    "                          .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, batch_loss/display_step))\n",
    "                batch_loss = 0\n",
    "                \n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    stop_early = 0\n",
    "                    \n",
    "                    print(\"New Record!\")\n",
    "                    saver = tf.train.Saver()\n",
    "                    saver.save(sess, save_path)\n",
    "                else:\n",
    "                    print(\"No Improvement for \" + str(stop_early) + \" Counts\")\n",
    "                    stop_early += 1\n",
    "    \n",
    "                update_loss = 0\n",
    "        \n",
    "    # Save Model\n",
    "#     saver = tf.train.Saver()\n",
    "#     saver.save(sess, save_path)\n",
    "#     print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\n",
      "l\n",
      "s\n",
      "s\n",
      "a\n",
      "</ACR>\n",
      "expertise\n",
      "aquatic\n",
      "training\n",
      "indigenous\n",
      "bronze\n",
      "surf\n",
      "injury\n",
      "sport\n",
      "federation\n",
      "recreation\n",
      "inland\n",
      "prevention\n",
      "research\n",
      "n\n",
      "swimming\n",
      "participation\n",
      "education\n",
      "pool\n",
      "guideline\n",
      "ambassador\n",
      "</TAG>\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "idx = randint(0, len(source_int_text)-1)\n",
    "source_predict = source_int_text[idx]\n",
    "for w in source_int_text[idx]:\n",
    "    print(int_to_vocab[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "royal\n",
      "life\n",
      "saving\n",
      "society\n",
      "australia\n",
      "</MEAN>\n"
     ]
    }
   ],
   "source": [
    "for w in target_int_text[idx]:\n",
    "    print(int_to_vocab[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom acronym test\n",
    "# cus_acr = \"mosfet\"\n",
    "# cus_tags = \"electronic circuit switch gate electricity component transistor source\"\n",
    "\n",
    "# cus_acr = \"blt\"\n",
    "# cus_tags = \"sandwich bread lunch dinner restaurant meal mustard cheese mayo\"\n",
    "\n",
    "cus_acr = \"bjt\"\n",
    "cus_tags = \"electronic circuit switch gate electricity component\"\n",
    "\n",
    "# cus_acr = \"fpga\"\n",
    "# cus_tags = \"electronic circuit integrated hardware language programmable logic blocks memory\"\n",
    "\n",
    "# cus_acr = \"asic\"\n",
    "# cus_tags = \"electronic circuit logic chip programmable hardware microprocessor memory rom ram eeprom\"\n",
    "\n",
    "# cus_acr = \"byob\"\n",
    "# cus_tags = \"party drinking alcohol invitation host guest\"\n",
    "\n",
    "\n",
    "\n",
    "source_predict = list(cus_acr) + ['</ACR>'] + cus_tags.split() + ['</TAG>']\n",
    "source_predict = [vocab_to_int[w] for w in source_predict if w in set(vocab_to_int.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Inputs\n",
      "  Word Ids: [16162, 12649, 16512, 2, 3585, 10722, 1162, 3317, 13798, 20778, 4]\n",
      "  Words:    ['b', 'j', 't', '</ACR>', 'electronic', 'circuit', 'switch', 'gate', 'electricity', 'component', '</TAG>']\n",
      "\n",
      "Prediction\n",
      "  Word Ids: [16982, 12169, 5614, 5]\n",
      "  Words:    blue jet technology </MEAN>\n"
     ]
    }
   ],
   "source": [
    "load_path = 'checkpoints/dev'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "    \n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_probability:0')\n",
    "    \n",
    "    pred_logits = sess.run(logits, {input_data: [source_predict]*batch_size, \n",
    "                                    target_sequence_length: [12]*batch_size, \n",
    "                                    source_sequence_length: [len(source_predict)]*batch_size, \n",
    "                                    keep_prob: 1.0})[0]\n",
    "    print('Inputs')\n",
    "    print('  Word Ids: {}'.format([i for i in source_predict]))\n",
    "    print('  Words:    {}'.format([int_to_vocab[i] for i in source_predict]))\n",
    "    print('\\nPrediction')\n",
    "    print('  Word Ids: {}'.format([i for i in pred_logits]))\n",
    "    print('  Words:    {}'.format(' '.join([int_to_vocab[i] for i in pred_logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
